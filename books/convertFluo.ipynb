{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import cv2\n",
    "import shutil \n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "from skimage import io, transform\n",
    "from skimage import color\n",
    "from skimage import morphology\n",
    "from skimage.morphology import binary_dilation\n",
    "import scipy.misc\n",
    "import scipy.ndimage as ndi\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from pytvision import visualization as view\n",
    "from pytvision.transforms import transforms as mtrans\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "from torchlib.datasets import dsxbdata\n",
    "from torchlib.datasets.dsxbdata import DSXBExDataset, DSXBDataset\n",
    "from torchlib.datasets import imageutl as imutl\n",
    "from torchlib import utils\n",
    "from torchlib.models import unetpad\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.style.use('fivethirtyeight')\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "from pytvision.transforms import transforms as mtrans\n",
    "from torchlib import metrics\n",
    "\n",
    "from torchlib.segneuralnet import SegmentationNeuralNet\n",
    "from torchlib import post_processing_func\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_simple_transforms(pad=0):\n",
    "    return transforms.Compose([\n",
    "        #mtrans.CenterCrop( (1008, 1008) ),\n",
    "        mtrans.ToPad( pad, pad, padding_mode=cv2.BORDER_CONSTANT ),\n",
    "        mtrans.ToTensor(),\n",
    "        normalize,      \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(src, titles=[], suptitle=\"\", \n",
    "         bwidth=4, bheight=4, save_file=False,\n",
    "         show_axis=True, show_cbar=False, last_max=0):\n",
    "\n",
    "    num_cols = len(src)\n",
    "    \n",
    "    plt.figure(figsize=(bwidth * num_cols, bheight))\n",
    "    plt.suptitle(suptitle)\n",
    "\n",
    "    for idx in range(num_cols):\n",
    "        plt.subplot(1, num_cols, idx+1)\n",
    "        if not show_axis: plt.axis(\"off\")\n",
    "        if idx < len(titles): plt.title(titles[idx])\n",
    "        \n",
    "        if idx == num_cols-1 and last_max:\n",
    "            plt.imshow(src[idx]*1, vmax=last_max, vmin=0)\n",
    "        else:\n",
    "            plt.imshow(src[idx]*1)\n",
    "        if type(show_cbar) is bool:\n",
    "            if show_cbar: plt.colorbar()\n",
    "        elif idx < len(show_cbar) and show_cbar[idx]:\n",
    "            plt.colorbar()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    if save_file:\n",
    "        plt.savefig(save_file)\n",
    "        \n",
    "class NormalizeInverse(torchvision.transforms.Normalize):\n",
    "    \"\"\"\n",
    "    Undoes the normalization and returns the reconstructed images in the input domain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean = (0.485, 0.456, 0.406), std  = (0.229, 0.224, 0.225)):\n",
    "        mean     = torch.as_tensor(mean)\n",
    "        std      = torch.as_tensor(std)\n",
    "        std_inv  = 1 / (std + 1e-7)\n",
    "        mean_inv = -mean * std_inv\n",
    "        super().__init__(mean=mean_inv, std=std_inv)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return super().__call__(tensor.clone())\n",
    "\n",
    "n = NormalizeInverse()\n",
    "\n",
    "\n",
    "normalize = mtrans.ToMeanNormalization(\n",
    "    mean = (0.485, 0.456, 0.406),  \n",
    "    std  = (0.229, 0.224, 0.225), \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdataset      = os.path.expanduser( '/home/chcp/Datasets' )\n",
    "#namedataset      = 'Seg33_1.0.3'\n",
    "#namedataset      = 'Bfhsc_1.0.0'\n",
    "namedataset      = 'FluoC2DLMSC_0.0.1'\n",
    "sub_folder       = 'val'\n",
    "folders_images   = 'images'\n",
    "folders_contours = 'touchs'\n",
    "folders_weights  = 'weights'\n",
    "folders_segment  = 'outputs'\n",
    "num_classes      = 4\n",
    "num_channels     = 3\n",
    "pad              = 0\n",
    "pathname         = pathdataset + '//' + namedataset\n",
    "subset           = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcs = ['unetpad', 'unet', 'unetresnet34', 'unetresnet101', 'segnet', 'albunet']\n",
    "nets = []\n",
    "\n",
    "for arch in arcs:\n",
    "    model_url_base = 'baseline_test'\n",
    "    pathmodel = r'/home/chcp/Code/pytorch-unet/out/Fluo/'\n",
    "    net = SegmentationNeuralNet(\n",
    "        patchproject=pathmodel, \n",
    "        nameproject=model_url_base, \n",
    "        no_cuda=True, parallel=False,\n",
    "        seed=2021, print_freq=False,\n",
    "        gpu=True\n",
    "        )\n",
    "\n",
    "    net.create( \n",
    "        arch=arch, \n",
    "        num_output_channels=num_classes, \n",
    "        num_input_channels=3,\n",
    "        loss='jreg', \n",
    "        lr=1e-3, \n",
    "        optimizer='adam',\n",
    "        lrsch='fixed',\n",
    "        )\n",
    "    nets.append(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'test'\n",
    "\n",
    "test_data = dsxbdata.ISBIDataset(\n",
    "    pathname, \n",
    "    subset, \n",
    "    folders_labels=f'labels{num_classes}c',\n",
    "    count=None,\n",
    "    num_classes=num_classes,\n",
    "    num_channels=num_channels,\n",
    "    transform=get_simple_transforms(pad=0),\n",
    "    use_weight=False,\n",
    "    weight_name='',\n",
    "    load_segments=False,\n",
    "    shuffle_segments=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in01 = np.zeros([1, 3, 832, 992])\n",
    "in02 = np.zeros([1, 3, 782, 1200])\n",
    "\n",
    "#in01 = np.zeros([1, 3, 832, 1024])\n",
    "#in02 = np.zeros([1, 3, 832, 1216])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input 1\n",
    "- (832; 992) -> (832; 1024)\n",
    "\n",
    "#### Input 2\n",
    "- (782; 1200) -> (832; 1216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = torch.zeros([1, 3, 832, 1216])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(lambda:[])\n",
    "for name, net in zip(arcs, nets):\n",
    "    results[name].append([])\n",
    "    try:\n",
    "        net(trial)\n",
    "        results[name].append('pass')\n",
    "    except: pass    \n",
    "results   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert False, 'just one time'\n",
    "urls = glob(new_data + '/test/outputs/*/*.tif')\n",
    "for url in urls:\n",
    "    \n",
    "    src = cv2.imread(url, -1)\n",
    "    dst = binary_dilation(src, selem=morphology.disk(1)) * 255\n",
    "    dst = dst.astype(np.uint8)\n",
    "    cv2.imwrite(url, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [00:07<00:00, 57.16it/s]\n"
     ]
    }
   ],
   "source": [
    "urls = glob(pathname+'/*/*/*.tif') + glob(pathname+'/*/*/*/*.tif')\n",
    "\n",
    "shapes = defaultdict(lambda:0)\n",
    "for url in tqdm(urls):\n",
    "    src = cv2.imread(url, -1)\n",
    "    \n",
    "    ini_h, ini_w = src.shape\n",
    "    \n",
    "    if src.shape == (832, 992): #832, 1024\n",
    "        target_h, target_w = 832, 1024\n",
    "        \n",
    "    if src.shape == (782, 1200): #832, 1216\n",
    "        target_h, target_w = 832, 1216\n",
    "    \n",
    "    diff_h = target_h - ini_h\n",
    "    diff_w = target_w - ini_w\n",
    "    \n",
    "    reflect101 = cv2.copyMakeBorder(src, diff_h//2, diff_h//2, diff_w//2, diff_w//2,cv2.BORDER_REFLECT_101)\n",
    "    \n",
    "    dst = url.replace(\"FluoC2DLMSC_0.0.1\", \"FluoC2DLMSC_0.1.1\")\n",
    "    Path(dst).parent.mkdir(exist_ok=True, parents=True)    \n",
    "    assert url!=dst\n",
    "    cv2.imwrite(dst, reflect101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../torchlib/datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weightmaps as wms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [21:58<00:00,  9.63s/it]\n"
     ]
    }
   ],
   "source": [
    "urls = glob('/home/chcp/Datasets/FluoC2DLMSC_0.1.1/*/labels4c/*')\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    src   = cv2.imread(url, -1)\n",
    "    \n",
    "    bwm   = wms.balancewm(src) \n",
    "    bwm_u = url.replace(\"labels4c\", 'weights/BWM').replace(\".tif\", '')\n",
    "        \n",
    "    dwm   = wms.distranfwm(src, 5)\n",
    "    dwm_u = url.replace(\"labels4c\", 'weights/DWM').replace(\".tif\", '')\n",
    "    \n",
    "    saw   = wms.shapeawewm(src, 5)\n",
    "    saw_u = url.replace(\"labels4c\", 'weights/SAW').replace(\".tif\", '')\n",
    "    \n",
    "    assert url != saw_u\n",
    "    assert url != bwm_u\n",
    "    assert url != dwm_u\n",
    "    \n",
    "    Path(bwm_u).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(dwm_u).parent.mkdir(parents=True, exist_ok=True)\n",
    "    Path(saw_u).parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez(dwm_u, dwm)\n",
    "    np.savez(saw_u, saw)\n",
    "    np.savez(bwm_u, bwm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
